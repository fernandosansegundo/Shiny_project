autoplot(CCF) +
ggtitle("Cross Correlation Function")
# 9.3 Fit initial FT model with large s
TF.fit <- arima(y,
order=c(1,0,0),
seasonal = list(order=c(1,0,0),period=12),
xtransf =  x,
transfer = list(c(0,7)), #List with (r,s) orders
include.mean = TRUE,
method="ML")
summary(TF.fit) # summary of training errors and estimated coefficients
coeftest(TF.fit) # statistical significance of estimated coefficients
#NOTE: If this regression error is not stationary in variance,boxcox should be applied to input and output series.
# Check numerator coefficients of explanatory variable
TF.Identification.plot(x,TF.fit)
# 8.2 Fit arima noise with selected
xlag = Lag(x,0)   # b
xlag[is.na(xlag)]=0
arima.fit <- arima(y,
order=c(1,1,0),
seasonal = list(order=c(1,1,0),period=12),
xtransf = xlag,
transfer = list(c(1,1)), #List with (r,s) orders
include.mean = FALSE,
method="ML")
summary(arima.fit) # summary of training errors and estimated coefficients
coeftest(arima.fit) # statistical significance of estimated coefficients
TF.RegressionError.plot(y, x, arima.fit, lag.max = 20)
# Check residuals
CheckResiduals.ICAI(arima.fit)
# If residuals are not white noise, change order of ARMA
ggtsdisplay(residuals(arima.fit),lag.max = 100)
# 8.3 Cross correlation residuals - expl. variable
res <- residuals(arima.fit)
res[is.na(res)] <- 0
ccf(y = res, x = x)
# Check residuals
CheckResiduals.ICAI(arima.fit)
arima.fit <- arima(y,
order=c(1,1,0),
seasonal = list(order=c(1,1,0),period=12),
xtransf = xlag,
transfer = list(c(1,1)), #List with (r,s) orders
include.mean = FALSE,
method="ML")
summary(arima.fit) # summary of training errors and estimated coefficients
coeftest(arima.fit) # statistical significance of estimated coefficients
TF.RegressionError.plot(y, x, arima.fit, lag.max = 20)
# Check residuals
CheckResiduals.ICAI(arima.fit)
TF.RegressionError.plot(y, x, arima.fit, lag.max = 20)
arima.fit <- arima(y,
order=c(2,1,0),
seasonal = list(order=c(1,1,0),period=12),
xtransf = xlag,
transfer = list(c(1,1)), #List with (r,s) orders
include.mean = FALSE,
method="ML")
summary(arima.fit) # summary of training errors and estimated coefficients
coeftest(arima.fit) # statistical significance of estimated coefficients
TF.RegressionError.plot(y, x, arima.fit, lag.max = 20)
arima.fit <- arima(y,
order=c(2,1,0),
seasonal = list(order=c(2,1,0),period=12),
xtransf = xlag,
transfer = list(c(1,1)), #List with (r,s) orders
include.mean = FALSE,
method="ML")
summary(arima.fit) # summary of training errors and estimated coefficients
coeftest(arima.fit) # statistical significance of estimated coefficients
TF.RegressionError.plot(y, x, arima.fit, lag.max = 20)
# Check residuals
CheckResiduals.ICAI(arima.fit)
# If residuals are not white noise, change order of ARMA
ggtsdisplay(residuals(arima.fit),lag.max = 100)
# Check residuals
CheckResiduals.ICAI(arima.fit)
# If residuals are not white noise, change order of ARMA
ggtsdisplay(residuals(arima.fit),lag.max = 100)
# 8.3 Cross correlation residuals - expl. variable
res <- residuals(arima.fit)
res[is.na(res)] <- 0
ccf(y = res, x = x)
# 8.4 Forecast for new data with h = 6 with the new values of the expl. variable
xnew <- read.table("unemploymentforecast.csv",header = TRUE, sep = ",")
#x_new <- ts(xnew[,c(2)])
x_new <- ts(xnew[,c(2)])
val.forecast_h6 <- TF.forecast(y.old = as.matrix(y), #past values of the series
x.old = as.matrix(xlag), #Past values of the explanatory variables
x.new = as.matrix(x_new), #New values of the explanatory variables
model = arima.fit, #fitted transfer function model
h=6) #Forecast horizon
val.forecast_h6 <- (val.forecast_h6)*10000
val.forecast_h6 <- data.frame("indexrow" = c(5:10),
"Mes" = Mes,
"Point Forecast" = val.forecast_h6)
fdata_last4v2 <- data.frame("indexrow" = c(1:4),
"Mes" =c("Aug 2020",  "Sep 2020", "Oct 2020", "Nov 2020"),
"Point Forecast" = c(tail(fdata, 4)[1 : 4, 2]))
fdata_last4v2 <- data.frame("indexrow" = c(1:4),
"Mes" =c("Aug 2020",  "Sep 2020", "Oct 2020", "Nov 2020"),
"Point Forecast" = c(tail(fdataint, 4)[1 : 4, 2]))
val.forecast_h6 <- rbind(val.forecast_h6, fdata_last4v2)
## Load file -------------------------------------------------------------------------------------
fdata <- read.table("PBCa.dat", sep = "", header = TRUE, stringsAsFactors = FALSE)
setwd("~/01. Máster Big Data/05. Machine Learning I/03. Exámenes/Plantillas Examen Mid-Term 2020")
## Load file -------------------------------------------------------------------------------------
fdata <- read.table("PBCa.dat", sep = "", header = TRUE, stringsAsFactors = FALSE)
str(fdata)
#Convert output variable to factor
fdata$Ya <- as.factor(fdata$Ya)
levels(fdata$Ya) <- make.names(levels(factor(fdata$Ya)))
str(fdata)
fdata = na.omit(fdata)
## Exploratory analysis -------------------------------------------------------------------------------------
ggplot(fdata) + geom_point(aes(x = X1, y = X2, color = Ya))
## 5. Exploratory analysis -------------------------------------------------------------------------------------
qqnorm(fdata$X1, pch = 1, frame = FALSE)
hist(fdata$x1)
hist(fdata$x1)
hist(fdata$X1)
X1_out = boxplot(fdata$X1)$out
qqline(fdata$X1, col = "steelblue", lwd = 2)
rm(fdata$X1, pch = 1, frame = FALSE)
qqline(fdata$X1,
## 4. Check for outliers ---------------------------------------------------------------
qqnorm(fdata$X1, pch = 1, frame = FALSE)
qqline(fdata$X1, col = "steelblue", lwd = 2)
## 4. Check for outliers ---------------------------------------------------------------
qqnorm(fdata$X1, pch = 1, frame = FALSE)
qqline(fdata$X1, col = "steelblue", lwd = 2)
library(car)
qqPlot(fdata$X1)
## 5. Standardization
scaleX1 <- scale(fdata$X1)
## 6. BoxCox transformation to reduce skewness --------------------------------------
BoxCox(fdata$X1)
## 6. BoxCox transformation to reduce skewness --------------------------------------
lambdaX1 <- BoxCox.lambda(fdata$X1)
## 6. BoxCox transformation to reduce skewness --------------------------------------
lambdascaleX1 <- BoxCox.lambda(fdata$scaleX1)
## 6. BoxCox transformation to reduce skewness --------------------------------------
lambdascaleX1 <- BoxCox.lambda(scaleX1)
scaletransformX1 <- BoxCox(scaleX1, lambdascaleX1)
scaleX1
hist(scaleX1)
hist(scaletransformX1)
hist(scaleX1)
ggplot(fdata) + geom_point(aes(x = X1, y = X2, color = Ya))
hist(scaleX1)
hist(scaletransformX1)
ggpairs(fdata,aes(color = Y, alpha = 0.3))
library(tidyverse)
library(GGally)
ggpairs(fdata,aes(color = Y, alpha = 0.3))
ggpairs(fdata,aes(color = Ya, alpha = 0.3))
## 8. If Class imbalances ----------------------------------------------------------
library(caret)
fdata2 <- downSample(fdata, fdata$Ya)
str(fdata2)
fdata[, -1]
fdata[, -4]
fdata2 <- downSample(fdata[, -4], fdata$Ya, yname = "Ya")
str(fdata2)
#Function for plotting multiple plots between the output of a data frame
#and the predictors of this output.
PlotDataframe(fdata = fdata,
output.name = "Ya")
PlotDataframe(fdata = fdata2,
output.name = "Ya")
scaleX2 <- scale(fdata$X2)
hist(scaleX2)
lambdascaleX2 <- BoxCox.lambda(scaleX2)
scaletransformX2 <- BoxCox(scaleX2, lambdascaleX2)
hist(scaleX2)
hist(scaletransformX2)
fdata2 <- downSample(fdata, fdata$Ya)
str(fdata2)
fdata2 <- downSample(fdata[, -4], fdata$Ya, yname = "Ya")
str(fdata2)
ggplot(fdata) + geom_point(aes(x = X1, y = X2, color = Ya))
PlotDataframe(fdata = fdata2,
output.name = "Ya")
#create random 80/20 % split
trainIndex <- createDataPartition(fdata$Ya,      #output variable. createDataPartition creates proportional partitions
p = 0.8,      #split probability for training
list = FALSE, #Avoid output as a list
times = 1)    #only one partition
## 9.1 obtain training and test sets -------------------------------------------------
fTR <- fdata[trainIndex,]
fTS <- fdata[-trainIndex,]
#Create dataset to include model predictions
fTR_eval <- fTR
fTS_eval <- fTS
#-------------------------------------------------------------------------------------------------
#---------------------------- LOGISTIC REGRESSION MODEL ----------------------------------------------
#-------------------------------------------------------------------------------------------------
## Train model -----------------------------------------------------------------------------------
set.seed(150) #For replication
#Train model using training data
LogReg.fit <- train(form = Ya ~ ., #formula for specifying inputs and outputs.
data = fTR,               #Training dataset
method = "glm",                   #Train logistic regression
preProcess = c("center","scale", "BoxCox"), #Center an scale inputs
trControl = ctrl,                 #trainControl Object
metric = "Accuracy")              #summary metric used for selecting hyperparameters
## Initialize trainControl -----------------------------------------------------------------------
ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
number = 10,                          #Number of folds
summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
classProbs = TRUE)                    #Compute class probs in Hold-out samples
#-------------------------------------------------------------------------------------------------
#---------------------------- LOGISTIC REGRESSION MODEL ----------------------------------------------
#-------------------------------------------------------------------------------------------------
## Train model -----------------------------------------------------------------------------------
set.seed(150) #For replication
#Train model using training data
LogReg.fit <- train(form = Ya ~ ., #formula for specifying inputs and outputs.
data = fTR,               #Training dataset
method = "glm",                   #Train logistic regression
preProcess = c("center","scale", "BoxCox"), #Center an scale inputs
trControl = ctrl,                 #trainControl Object
metric = "Accuracy")              #summary metric used for selecting hyperparameters
LogReg.fit          #information about the resampling
summary(LogReg.fit) #detailed information about the fit of the final model
LogReg.fit          #information about the resampling
plot(LogReg.fit)
LogReg.fit          #information about the resampling
summary(LogReg.fit) #detailed information about the fit of the final model
str(LogReg.fit)     #all information stored in the model
## Understanding resampling methods -------------------------------------------
str(LogReg.fit$control$index)       #Training indexes
str(LogReg.fit$control$indexOut)    #Test indexes
LogReg.fit$resample                 #Resample test results
## Evaluate model --------------------------------------------------------------------------------
#Evaluate the model with training and test sets
#training
fTR_eval$LRprob <- predict(LogReg.fit, type="prob", newdata = fTR) # predict probabilities
fTR_eval$LRpred <- predict(LogReg.fit, type="raw", newdata = fTR) # predict classes
#test
fTS_eval$LRprob <- predict(LogReg.fit, type="prob", newdata = fTS) # predict probabilities
fTS_eval$LRpred <- predict(LogReg.fit, type="raw", newdata = fTS) # predict classes
#######confusion matices
# Training
confusionMatrix(data = fTR_eval$LRpred, #Predicted classes
reference = fTR_eval$Y, #Real observations
positive = "YES") #Class labeled as Positive
#######confusion matices
# Training
confusionMatrix(data = fTR_eval$LRpred, #Predicted classes
reference = fTR_eval$Ya, #Real observations
positive = "YES") #Class labeled as Positive
#######confusion matices
# Training
confusionMatrix(data = fTR_eval$LRpred, #Predicted classes
reference = fTR_eval$Y)
# test
confusionMatrix(fTS_eval$LRpred,
fTS_eval$Y)
#-------------------------------------------------------------------------------------------------
#---------------------------- KNN MODEL  ----------------------------------------------
#-------------------------------------------------------------------------------------------------
set.seed(150) #For replication
#Train knn model model.
#Knn contains 1 tuning parameter k (number of neigbors). Three options:
#  - Train with a fixed parameter: tuneGrid = data.frame(k = 5),
#  - Try with a range of values specified in tuneGrid: tuneGrid = data.frame(k = seq(2,120,4)),
#  - Caret chooses 10 values: tuneLength = 10,
knn.fit = train(form = Ya ~ ., #formula for specifying inputs and outputs.
data = fTR,   #Training dataset
method = "knn",
preProcess = c("center","scale", "BoxCox"),
#tuneGrid = data.frame(k = 5),
tuneGrid = data.frame(k = seq(1, 50),2),
#tuneLength = 10,
trControl = ctrl,
metric = "Accuracy")
#-------------------------------------------------------------------------------------------------
#---------------------------- KNN MODEL  ----------------------------------------------
#-------------------------------------------------------------------------------------------------
set.seed(150) #For replication
#Train knn model model.
#Knn contains 1 tuning parameter k (number of neigbors). Three options:
#  - Train with a fixed parameter: tuneGrid = data.frame(k = 5),
#  - Try with a range of values specified in tuneGrid: tuneGrid = data.frame(k = seq(2,120,4)),
#  - Caret chooses 10 values: tuneLength = 10,
knn.fit = train(form = Ya ~ ., #formula for specifying inputs and outputs.
data = fTR,   #Training dataset
method = "knn",
preProcess = c("center","scale", "BoxCox"),
#tuneGrid = data.frame(k = 5),
tuneGrid = data.frame(k = seq(1,50,2)),
#tuneLength = 10,
trControl = ctrl,
metric = "Accuracy")
knn.fit #information about the settings
ggplot(knn.fit) #plot the summary metric as a function of the tuning parameter
knn.fit$finalModel #information about final model trained
#-------------------------------------------------------------------------------------------------
#---------------------------- DECISION TREE ------------------------------------------------------
#-------------------------------------------------------------------------------------------------
library(rpart)
library(rpart.plot)
library(partykit)
fTR <- fdata[trainIndex,]
fTS <- fdata[-trainIndex,]
inputs <- 1:2
set.seed(150) #For replication
inputs <- 1:3
set.seed(150) #For replication
#NOTE: Formula method could be used, but it will automatically create dummy variables.
# Decision trees can work with categorical variables as theey are. Then, x and y arguments are used
tree.fit <- train(x = fTR[,inputs],  #Input variables.
y = fTR$Ya,   #Output variable
method = "rpart",   #Decision tree with cp as tuning parameter
control = rpart.control(minsplit = 5,  # Minimum number of obs in node to keep cutting
minbucket = 5), # Minimum number of obs in a terminal node
parms = list(split = "gini"),          # impuriry measure
#tuneGrid = data.frame(cp = 0.1), # TRY this:
#tuneGrid = data.frame(cp = 0.25),
#tuneLength = 10,
#tuneGrid = data.frame(cp = seq(0,0.1,0.0005))
tuneGrid = data.frame(cp = seq(0,0.5,0.05)),
trControl = ctrl,
metric = "Accuracy")
tree.fit #information about the resampling settings
ggplot(tree.fit) #plot the summary metric as a function of the tuning parameter
#NOTE: Formula method could be used, but it will automatically create dummy variables.
# Decision trees can work with categorical variables as theey are. Then, x and y arguments are used
tree.fit <- train(x = fTR[,inputs],  #Input variables.
y = fTR$Ya,   #Output variable
method = "rpart",   #Decision tree with cp as tuning parameter
control = rpart.control(minsplit = 5,  # Minimum number of obs in node to keep cutting
minbucket = 5), # Minimum number of obs in a terminal node
parms = list(split = "gini"),          # impuriry measure
#tuneGrid = data.frame(cp = 0.1), # TRY this:
#tuneGrid = data.frame(cp = 0.25),
#tuneLength = 10,
#tuneGrid = data.frame(cp = seq(0,0.1,0.0005))
tuneGrid = data.frame(cp = seq(0,10,1)),
trControl = ctrl,
metric = "Accuracy")
tree.fit #information about the resampling settings
ggplot(tree.fit) #plot the summary metric as a function of the tuning parameter
summary(tree.fit)  #information about the model trained
tree.fit$finalModel #Cuts performed and nodes. Also shows the number and percentage of cases in each node.
#Basic plot of the tree:
plot(tree.fit$finalModel, uniform = TRUE, margin = 0.1)
text(tree.fit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
#Advanced plots
rpart.plot(tree.fit$finalModel, type = 2, fallen.leaves = FALSE, box.palette = "Oranges")
tree.fit.party <- as.party(tree.fit$finalModel)
plot(tree.fit.party)
#Measure for variable importance
varImp(tree.fit,scale = FALSE)
plot(varImp(tree.fit,scale = FALSE))
#NOTE: Formula method could be used, but it will automatically create dummy variables.
# Decision trees can work with categorical variables as theey are. Then, x and y arguments are used
tree.fit <- train(x = fTR[,inputs],  #Input variables.
y = fTR$Ya,   #Output variable
method = "rpart",   #Decision tree with cp as tuning parameter
control = rpart.control(minsplit = 5,  # Minimum number of obs in node to keep cutting
minbucket = 5), # Minimum number of obs in a terminal node
parms = list(split = "gini"),          # impuriry measure
#tuneGrid = data.frame(cp = 0.1), # TRY this:
#tuneGrid = data.frame(cp = 0.25),
#tuneLength = 10,
#tuneGrid = data.frame(cp = seq(0,0.1,0.0005))
tuneGrid = data.frame(cp = seq(1,10,1)),
trControl = ctrl,
metric = "Accuracy")
tree.fit #information about the resampling settings
ggplot(tree.fit) #plot the summary metric as a function of the tuning parameter
#NOTE: Formula method could be used, but it will automatically create dummy variables.
# Decision trees can work with categorical variables as theey are. Then, x and y arguments are used
tree.fit <- train(x = fTR[,inputs],  #Input variables.
y = fTR$Ya,   #Output variable
method = "rpart",   #Decision tree with cp as tuning parameter
control = rpart.control(minsplit = 5,  # Minimum number of obs in node to keep cutting
minbucket = 5), # Minimum number of obs in a terminal node
parms = list(split = "gini"),          # impuriry measure
#tuneGrid = data.frame(cp = 0.1), # TRY this:
#tuneGrid = data.frame(cp = 0.25),
#tuneLength = 10,
#tuneGrid = data.frame(cp = seq(0,0.1,0.0005))
tuneGrid = data.frame(cp = seq(1,100,1)),
trControl = ctrl,
metric = "Accuracy")
tree.fit #information about the resampling settings
ggplot(tree.fit) #plot the summary metric as a function of the tuning parameter
#NOTE: Formula method could be used, but it will automatically create dummy variables.
# Decision trees can work with categorical variables as theey are. Then, x and y arguments are used
tree.fit <- train(#x = fTR[,inputs],  #Input variables.
y = fTR$Ya,   #Output variable
data = fTR,
method = "rpart",   #Decision tree with cp as tuning parameter
control = rpart.control(minsplit = 5,  # Minimum number of obs in node to keep cutting
minbucket = 10), # Minimum number of obs in a terminal node
parms = list(split = "gini"),          # impuriry measure
#tuneGrid = data.frame(cp = 0.1), # TRY this:
#tuneGrid = data.frame(cp = 0.25),
#tuneLength = 10,
#tuneGrid = data.frame(cp = seq(0,0.1,0.0005))
tuneGrid = data.frame(cp = seq(1,10,1)),
trControl = ctrl,
metric = "Accuracy")
#NOTE: Formula method could be used, but it will automatically create dummy variables.
# Decision trees can work with categorical variables as theey are. Then, x and y arguments are used
tree.fit <- train(x = fTR[,inputs],  #Input variables.
y = fTR$Ya,   #Output variable
data = fTR,
method = "rpart",   #Decision tree with cp as tuning parameter
control = rpart.control(minsplit = 5,  # Minimum number of obs in node to keep cutting
minbucket = 10), # Minimum number of obs in a terminal node
parms = list(split = "gini"),          # impuriry measure
#tuneGrid = data.frame(cp = 0.1), # TRY this:
#tuneGrid = data.frame(cp = 0.25),
#tuneLength = 10,
#tuneGrid = data.frame(cp = seq(0,0.1,0.0005))
tuneGrid = data.frame(cp = seq(1,10,1)),
trControl = ctrl,
metric = "Accuracy")
tree.fit #information about the resampling settings
ggplot(tree.fit) #plot the summary metric as a function of the tuning parameter
fit.decissiontree <- train(Ya ~ ., data = fTR, method = "rpart",
trControl = ctrl,
# preProcess = c("center","scale"),
# tuneGrid =data.frame(cp=0.05))
tuneLength = 30,
metric="Accuracy")
fit.decissiontree
ggplot(fit.decissiontree)
fit.decissiontree <- train(Ya ~ ., data = fTR, method = "rpart",
trControl = ctrl,
# preProcess = c("center","scale"),
# tuneGrid =data.frame(cp=0.05))
tuneLength = 100,
metric="Accuracy")
fit.decissiontree
ggplot(fit.decissiontree)
summary(tree.fit)
fit.decissiontree
ggplot(fit.decissiontree)
summary(fit.decissiontree)
fit.decissiontree$finalModel #Cuts performed and nodes. Also shows the number and percentage of cases in each node.
#Basic plot of the tree:
plot(fit.decissiontree$finalModel, uniform = TRUE, margin = 0.1)
text(fit.decissiontree$finalModel, use.n = TRUE, all = TRUE, cex = .8)
#Measure for variable importance
varImp(fit.decissiontree,scale = FALSE)
plot(varImp(fit.decissiontree,scale = FALSE))
fit.decissiontree <- train(Ya ~ ., data = fTR, method = "rpart",
trControl = ctrl,
# preProcess = c("center","scale"),
# tuneGrid =data.frame(cp=0.05))
tuneLength = 100,
metric="Kappa")
fit.decissiontree
ggplot(fit.decissiontree)
summary(fit.decissiontree)
## Initialize trainControl -----------------------------------------------------------------------
ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
number = 10,                          #Number of folds
summaryFunction = multiClassSummary(),     #Performance summary for comparing models in hold-out samples.
classProbs = TRUE)                    #Compute class probs in Hold-out samples
## Initialize trainControl -----------------------------------------------------------------------
ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
number = 10,                          #Number of folds
summaryFunction = multiClassSummary,     #Performance summary for comparing models in hold-out samples.
classProbs = TRUE)                    #Compute class probs in Hold-out samples
fit.decissiontree <- train(Ya ~ ., data = fTR, method = "rpart",
trControl = ctrl,
# preProcess = c("center","scale"),
# tuneGrid =data.frame(cp=0.05))
tuneLength = 100,
metric="Kappa")
## Initialize trainControl -----------------------------------------------------------------------
library(MLmetrics)
install.packages("MLmetrics")
## Initialize trainControl -----------------------------------------------------------------------
library(MLmetrics)
ctrl <- trainControl(method = "cv",                        #k-fold cross-validation
number = 10,                          #Number of folds
summaryFunction = multiClassSummary,     #Performance summary for comparing models in hold-out samples.
classProbs = TRUE)                    #Compute class probs in Hold-out samples
fit.decissiontree <- train(Ya ~ ., data = fTR, method = "rpart",
trControl = ctrl,
# preProcess = c("center","scale"),
# tuneGrid =data.frame(cp=0.05))
tuneLength = 100,
metric="Kappa")
fit.decissiontree
ctrl2 <- trainControl(method = "cv",                        #k-fold cross-validation
number = 10,                          #Number of folds
summaryFunction = defaultSummary,     #Performance summary for comparing models in hold-out samples.
classProbs = TRUE)                    #Compute class probs in Hold-out samples
setwd("~/01. Máster Big Data/01. Fundamentos Matemáticos del Análisis de Datos/02. Proyectos")
library(shiny); runApp('Shiny_App_Test.R')
setwd("~/01. Máster Big Data/01. Fundamentos Matemáticos del Análisis de Datos/02. Proyectos/Shiny_App")
runApp('Shiny_App.R')
#Cargamos librerías
install.packages('rsconnect')
install.packages("rsconnect")
rsconnect::setAccountInfo(name='jorgegomez94',
token='A290297931857A3098C5825A2BF17F81',
secret='<SECRET>')
rsconnect::setAccountInfo(name='jorgegomez94',
token='A290297931857A3098C5825A2BF17F81',
secret='dYliMecLfrhiTGgITPivS+2jGNiRqiKKd46MBCdZ')
library(shiny); runApp('Shiny_App.R')
runApp('Shiny_App.R')
library(shiny); runApp('Shiny_App.R')
